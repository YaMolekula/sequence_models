{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767\n",
    "DEBUG = True\n",
    "\n",
    "import re, random, math, csv, io, string, itertools, sys\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = dict(\n",
    "    n_layers = 2,\n",
    "    hidden_size = 512,\n",
    "    fc_size = 512,\n",
    "    dropout = 0.9,\n",
    "    batch_size = 20,\n",
    "    lr = 0.001,\n",
    "    lr_decay = 0.9999,\n",
    "    min_lr = 0.00001,\n",
    "    grad_clip = 5.,\n",
    "    cuda = False,\n",
    "    num_epoch = 5,\n",
    "    max_length = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voc:\n",
    "    SOS = \"!\"\n",
    "    EOS = \"#\"\n",
    "    SOS_ID = 0\n",
    "    EOS_ID = 1\n",
    "    def __init__(self):\n",
    "        self.word2index = {self.SOS:0, self.EOS:1}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:self.SOS, 1:self.EOS}\n",
    "        self.n_words = 2 # Count SOS and EOS\n",
    "\n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "def string2indicies(voc, text):\n",
    "    return [voc.word2index[c] for c in text]\n",
    "    \n",
    "def indicies2string(voc, indicies):\n",
    "    return \"\".join([voc.index2word[i] for i in indicies])\n",
    "\n",
    "voc = Voc()\n",
    "\n",
    "for c in itertools.chain(range(ord('a'), ord('z')+1),range(ord('A'),ord('Z')+1),(ord(\" \"),)):\n",
    "    voc.index_word(chr(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairGenerator:\n",
    "#     vocabulary = [chr(i) for i in itertools.chain(range(ord('a'), ord('z')+1),range(ord('A'),ord('Z')+1))]\n",
    "    word_len_interval = {\"a\":2,\"b\":7}\n",
    "    sent_len_interval = {\"a\":1,\"b\":10}\n",
    "    \n",
    "    def __init__(self,voc):\n",
    "        self.voc = voc\n",
    "        self.vocabulary = [c for c in voc.word2index.keys() if c not in {voc.SOS, voc.EOS}]\n",
    "        \n",
    "    def gen_word_pair(self):\n",
    "        word_len = int(random.uniform(**self.word_len_interval))\n",
    "        word = np.random.choice(self.vocabulary,word_len)\n",
    "        return \"\".join(word), \"\".join(list(reversed(word)))\n",
    "    \n",
    "    def gen_pair(self):\n",
    "        num_words = int(random.uniform(**self.sent_len_interval))\n",
    "        inp, out = zip(*[self.gen_word_pair() for _ in range(num_words)])\n",
    "        return self.voc.SOS+\" \".join(inp)+self.voc.EOS, self.voc.SOS+\" \".join(out)+self.voc.EOS\n",
    "    \n",
    "    def gen_batch(self, n_unrollings):\n",
    "        inp, out = zip(*[self.gen_pair() for _ in range(n_unrollings)])\n",
    "        return inp, out\n",
    "    \n",
    "    def gen_int_batch(self, n_unrollings):\n",
    "        inp, out = zip(*[self.gen_pair() for _ in range(n_unrollings)])\n",
    "        \n",
    "        return \\\n",
    "            [string2indicies(self.voc, x) for x in inp], \\\n",
    "            [string2indicies(self.voc, x) for x in out]\n",
    "        \n",
    "pg = PairGenerator(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_unrollings=100, echo_step=2, batch_size=5):\n",
    "    x = np.array(np.random.choice(2, n_unrollings, p=[0.5, 0.5]))\n",
    "    y = np.roll(x, echo_step)\n",
    "    y[0:echo_step] = 0\n",
    "\n",
    "    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
    "    y = y.reshape((batch_size, -1))\n",
    "\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    hp = dict(\n",
    "        state_sz=5,\n",
    "        n_classes=2,\n",
    "        input_dim=2,\n",
    "        ckpt_path=\"./checkpoints/\"\n",
    "    )\n",
    "    def __init__(self, **hyper_parameters):\n",
    "        if hyper_parameters is not None:\n",
    "            for k, v in hyper_parameters.items():\n",
    "                self.hp[k] = v\n",
    "        self.__graph__()\n",
    "    \n",
    "    def _init(self):\n",
    "        self.x = tf.placeholder(tf.int32, [None, None], \"x\") #batch*voc\n",
    "        self.y = tf.placeholder(tf.int32, [None], \"y\") #batch*voc\n",
    "        emb = tf.placeholder(tf.int32, [None, None],)\n",
    "        \n",
    "        # batch_sz*seq_len -> batch_sz*seq_ln*voc_sz\n",
    "        embs = tf.get_variable('emb', [self.hp['n_classes'], self.hp['state_sz']]) \n",
    "        self.rnn_inputs = tf.nn.embedding_lookup(embs, x)\n",
    "        \n",
    "        #batch*state_sz\n",
    "        self.init_state = tf.placeholder(tf.float32, [None, self.hp['state_sz']], \"init_state\")\n",
    "    \n",
    "    def _weights(self):\n",
    "        self.wz = tf.get_variable(\n",
    "            \"wz\", shape=[self.hp['state_sz'], self.hp['state_sz']],\n",
    "            initializer=tf.contrib.layers.xavier_initializer()\n",
    "        )\n",
    "        self.uz = tf.get_variable(\n",
    "            \"uz\", shape=[self.hp['state_sz'], self.hp['state_sz']],\n",
    "            initializer=tf.contrib.layers.xavier_initializer()\n",
    "        )\n",
    "        self.bz = tf.get_variable(\n",
    "            \"bz\", shape=[self.hp['state_sz']],\n",
    "            initializer=tf.constant_initializer(0.)\n",
    "        )\n",
    "        \n",
    "        self.wr = tf.get_variable(\n",
    "            \"wr\",  shape=[self.hp['state_sz'], self.hp['state_sz']],\n",
    "            initializer=tf.contrib.layers.xavier_initializer()\n",
    "        )\n",
    "        self.ur = tf.get_variable(\n",
    "            \"ur\",  shape=[self.hp['state_sz'], self.hp['state_sz']],\n",
    "            initializer=tf.contrib.layers.xavier_initializer()\n",
    "        )\n",
    "        self.br = tf.get_variable(\n",
    "            \"br\", shape=[self.hp['state_sz']],\n",
    "            initializer=tf.constant_initializer(0.)\n",
    "        )\n",
    "        \n",
    "        self.wh = tf.get_variable(\n",
    "            \"wh\",  shape=[self.hp['state_sz'], self.hp['state_sz']],\n",
    "            initializer=tf.contrib.layers.xavier_initializer()\n",
    "        )\n",
    "        self.uh = tf.get_variable(\n",
    "            \"uh\",  shape=[self.hp['state_sz'], self.hp['state_sz']],\n",
    "            initializer=tf.contrib.layers.xavier_initializer()\n",
    "        )\n",
    "        self.bh = tf.get_variable(\n",
    "            \"bh\", shape=[self.hp['state_sz']],\n",
    "            initializer=tf.constant_initializer(0.)\n",
    "        )\n",
    "        \n",
    "        # layer to decode results of GRU \n",
    "        self.wo= tf.get_variable(\n",
    "            'wo', shape=[self.hp['state_sz'], self.hp['n_classes']], \n",
    "            initializer=tf.contrib.layers.xavier_initializer()\n",
    "        )\n",
    "        self.bo = tf.get_variable(\n",
    "            'bo', shape=[self.hp['n_classes']], \n",
    "            initializer=tf.constant_initializer(0.)\n",
    "        )\n",
    "    \n",
    "    def __graph__(self):\n",
    "        # time cycle step\n",
    "        def step(prev_state, x):\n",
    "            z = tf.sigmoid(\n",
    "                tf.matmul(x,self.wz) + tf.matmul(prev_state, self.uz) + self.bz\n",
    "            )\n",
    "            r = tf.sigmoid(\n",
    "                tf.matmul(x,self.wr) + tf.matmul(prev_state, self.ur) + self.br\n",
    "            )\n",
    "            h = tf.tanh(\n",
    "                tf.matmul(x,self.wh) + tf.matmul(r*prev_state, self.uh) + self.bh\n",
    "            )\n",
    "            return (1-z)*prev_state + z*h\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        self._init()\n",
    "        self._weights()\n",
    "        states = tf.scan(\n",
    "            step,\n",
    "            #batch_sz*seq_ln*voc_sz -> seq_len*batch_sz*voc_sz\n",
    "            tf.transpose(self.rnn_inputs,[1,0,2]),\n",
    "            initializer=self.init_state\n",
    "        )\n",
    "        # seq_len*batch_sz*voc_sz -> batch_sz*seq_ln*voc_sz\n",
    "        states = tf.transpose(states,[1,0,2])\n",
    "        \n",
    "        self.states_reshaped = tf.reshape(states, [-1, self.hp['state_sz']])\n",
    "        logits = tf.matmul(self.states_reshaped, self.wo) + self.bo\n",
    "        \n",
    "        self.last_state = states[-1]\n",
    "        self.predictions = tf.nn.softmax(logits)\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=self.y)\n",
    "        )\n",
    "        self.train_op = tf.train.AdagradOptimizer(learning_rate=0.2).minimize(self.loss)\n",
    "        \n",
    "    def train(self, batch_generator, n_epochs = 10):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for epoch_num in range(n_epochs):\n",
    "                train_loss = 0\n",
    "                try:\n",
    "                    x,y = batch_generator()\n",
    "                    batch_sz = x.shape[0]\n",
    "                    \n",
    "                    tmp = sess.run(\n",
    "                        [self.states_reshaped], \n",
    "                        feed_dict = {\n",
    "                            self.x : x,\n",
    "                            self.y : y.flatten(),\n",
    "                            self.init_state : np.zeros(\n",
    "                                [batch_sz, self.hp['state_sz']]\n",
    "                            )\n",
    "                        }\n",
    "                    )\n",
    "                    sys.stdout.write(f\"{np.shape(a=tmp)} \")\n",
    "#                     _, train_loss_, tmp = sess.run(\n",
    "#                         [self.train_op, self.loss,self.states_reshaped], \n",
    "#                         feed_dict = {\n",
    "#                             self.x : x,\n",
    "#                             self.y : y.flatten(),\n",
    "#                             self.init_state : np.zeros(\n",
    "#                                 [batch_sz, self.hp['state_sz']]\n",
    "#                             )\n",
    "#                         }\n",
    "#                     )\n",
    "#                     sys.stdout.write(f\"{train_loss_:.3f} {tmp.shape} \")\n",
    "                except KeyboardInterrupt as ex:\n",
    "                    print(\"Interrupted by user at\")\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(sess, self.hp['ckpt_path'] + \"nn.mdl\", global_step=epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000, 5) (1, 1000, 5) (1, 1000, 5) (1, 1000, 5) (1, 1000, 5) (1, 1000, 5) (1, 1000, 5) (1, 1000, 5) (1, 1000, 5) (1, 1000, 5) "
     ]
    }
   ],
   "source": [
    "gru = GRU()\n",
    "gru.train(lambda: generate_data(n_unrollings=500,batch_size=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=generate_data(n_unrollings=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 200)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 200)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
